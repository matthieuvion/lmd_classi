{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7985a4c-5652-4da5-9277-d7465724811f",
   "metadata": {},
   "source": [
    "# Optional : replace LogisticRegression head and quick/partial fix out of memory errors (OOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7877830-5fa4-45fb-94d7-17baf1ec1419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments, sample_dataset\n",
    "\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "from optuna import Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b8c40-d3ed-4664-ad93-5cefc7a587f6",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c817176-48f0-45e0-bbc6-028a8ca6869e",
   "metadata": {},
   "source": [
    "#### Load from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c438543f-bb46-43f1-abb3-74abccefd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/lmd_ukraine_annotated.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b1659a-ab2d-4a95-a26d-957b3149c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(filepath)\n",
    "display(data.head(3))\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed28467-3dd2-4cef-9439-2e8c17c3b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For later stage and to comply with huggingface Dataset format, convert article_type to string type\n",
    "data['article_type'] = data['article_type'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a2b3b-c478-4d5f-a651-03e7155b68bd",
   "metadata": {},
   "source": [
    "#### Classes overview / % annotated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894005d2-600d-4568-8ab4-dbf1a54a6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "print(data.classe.value_counts())\n",
    "print(sum(data.classe.notnull()))\n",
    "print(sum(data.classe.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9840b5c0-2f4a-4a12-a6c5-94d0509f7e60",
   "metadata": {},
   "source": [
    "## Prepare Dataset (labels, optional sample, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a07548-cfd7-4ad9-bbbc-b1b6da0c0b8c",
   "metadata": {},
   "source": [
    "#### Split, convert to Huggingface DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5abd6-50c2-4100-a806-2b32bf34e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select labeled data only to split between train and eval, test set is the unlabeled data.\n",
    "with_labels = data.query(\"classe.notnull()\")\n",
    "test_df = data.query(\"classe.isnull()\")\n",
    "print(len(with_labels), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7c972-0b17-4a46-a5c3-70dbb04973b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled data is split between train and eval sets\n",
    "# Optional stratify= but we still want to make sure classes are \"balanced\" in both dataset\n",
    "\n",
    "train_df, eval_df = train_test_split(with_labels, test_size=0.4, stratify=with_labels['classe'], random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616afe2-da40-4b43-8079-2b1b3078d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make sure the smaller class has enough labels (e.g 8, or 20 or 50 or \"max\" 100).of\n",
    "# This dataset will later be sampled again using Setfit.sample_dataset. Classes will have the same amount of rows (8 or 10 or 60...)\n",
    "print(len(train_df))\n",
    "print(train_df.classe.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4fca4-d8db-4901-a50f-5676ae6ebc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(eval_df))\n",
    "eval_df.classe.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be4f66-afaa-461c-bdf0-a167a959354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For labeled data, add a 'label' column where 'classe' labels strings -> int\n",
    "# We do it now, because we SetFit wants integers and not floats for training\n",
    "label_mapping = {'pro_ukraine': 0, 'pro_russia': 1, 'other': 2}\n",
    "for df in [train_df, eval_df]:\n",
    "    df['label'] = df['classe'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e809c-5b2f-425d-98b0-b83115fbf41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f85376-e7e0-4d34-a2f1-cad6ffe1690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to huggingface --commonly used, DatasetDict format\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': eval_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba4f7d-f665-43bf-b846-b546420d2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save # classes, to be used later when loading model\n",
    "num_classes = len(train_dataset.unique(\"label\"))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24c0a2-4f9e-4429-b513-a84b820261f1",
   "metadata": {},
   "source": [
    "## Modeling/HPO : replace LogisticRegression head by GradientBoosting (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d6f7c-972e-49b3-b5b5-d027ea54c7de",
   "metadata": {},
   "source": [
    "Setfit docs recommends the sklearn logistic regression head though (see option B.). Performs a bit better in our use case too.  \n",
    "Here, by specifying use_differentiable_head=True, `SetFitHead`, a custom torch classification head is used.  \n",
    "To use your own custom classification head see [here](https://huggingface.co/docs/setfit/how_to/classification_heads).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bd279-4bb4-4738-98d8-35b162f03fed",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59863911-8e9b-4595-8f32-80c694f398bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SetFitModel.from_pretrained(\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    use_differentiable_head=True, head_params={\"out_features\": num_classes})\n",
    "model.model_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40ad2c-b49d-49c0-9e74-240c90b47963",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.labels = [\"pro_ukraine\", \"pro_russia\", \"other\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a20a4-ed05-41a2-9bf7-fb2a335a18f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Set Trainer args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14671f99-90ed-4df2-a516-5e7040aacbef",
   "metadata": {},
   "source": [
    "Might try to play on [sampling_strategy](https://huggingface.co/docs/setfit/v1.0.0/en/reference/trainer#setfit.TrainingArguments) (i.e undersampling or unique) for minority class \"pro_russian\".  \n",
    "From SetFit doc, num_epochs, max_steps and body_learning_rate are the most important regarding phase 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e7dcf-feb0-4414-9ba2-8dc98e167e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize your training arguments here, setfit.TrainingArguments class\n",
    "# tuples correspond to steps 1. finetuning embedding, 2. training classification head\n",
    "args = TrainingArguments(\n",
    "    batch_size=(32, 16), # default is (16,2), second value is for the classification head (SetFitHead)\n",
    "    num_epochs=(1, 16), # default (1, 16)\n",
    "    end_to_end=True, # if False (default), freezes body and train Head only. If True train the entire model during the classi. phase.\n",
    "    body_learning_rate=(2e-5, 1e-5), # (2e-5, 1e-5) by default. Only used if end to end is True (else body is frozen)\n",
    "    head_learning_rate=2e-3, # default 1e-2\n",
    "    l2_weight=0.01, # optional weight model body & head, passed to AdamW optimizer in classification training\n",
    "    sampling_strategy='oversampling', # default is oversampling. Kinda replace --but still exist, the num_iterations args\n",
    "    max_steps=-1 # default -1. Can also overrides num_epochs and reduce the # steps that would be otherwise needed.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413064e9-04dd-45f7-b104-075449d2d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the trainer.  \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    metric='accuracy', #default\n",
    "    column_mapping={\"comment\": \"text\", \"label\": \"label\"}, # cols expected by the model   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813a4804-3c8a-4b0f-a592-280caae38e44",
   "metadata": {},
   "source": [
    "#### Fine-tune (embeddings, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff392dc6-89a3-4a37-a19b-4ba1abf12c7e",
   "metadata": {},
   "source": [
    "In Setfit version >= 1, no need to freeze/unfreeze the head, the two steps 1.fine tune embeddings 2. classifier are done automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed037e3-9281-4f96-9b66-8a5997193a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c759c-7591-4cdc-b097-7473fccfd483",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68472f2-5ced-4f31-87ca-5bc32912653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c5826-8caa-4f16-bd4f-913d79cf79f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpnet\n",
    "# NO sample_dataset, samp_strat='oversampling', batch_size=(32, 16), num_epochs=(1, 16), body_learning_rate=(2e-5, 1e-5), l2_weight=0.01\n",
    "# result : 66%\n",
    "\n",
    "# NO sample_dataset, samp_strat='undersampling', batch_size=(16, 2), num_epochs=(*2*, 16), body_learning_rate=(2e-5, 1e-5), l2_weight=None\n",
    "# result : 64.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45024c0f-db0c-404b-a64c-a4e0ac414da9",
   "metadata": {},
   "source": [
    "Recommended method over SetFitHead (SetFit customized torch classifier head).  \n",
    "Previous differentiable head params was removed.  \n",
    "Additional params can be specified using `head_params` or a customized head can implemented manually (cf. doc).  \n",
    "Check sklearn [LogisticRegression module](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for more params (solver, max_iter, class_weight etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56d678a-f9ce-4096-b1c1-989e148eb064",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2027e3-9bb1-4a33-bd12-0603bc672986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to accelerate our tests : 16 examples per class, -> 48 rows -> x generated examples for contrastive learning\n",
    "train_dataset = sample_dataset(dataset[\"train\"], label_column=\"label\", num_samples=16, seed=40)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec9b31-c66d-4318-8b5a-fa0721eb531d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SetFitModel.from_pretrained(\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    head_params={\n",
    "        \"solver\": \"liblinear\", # default is liblinear. Other choices :  lbfgs, saga, newton-cg etc.\n",
    "        \"max_iter\": 250, # default is 100\n",
    "        \"class_weight\": None  # default None, try 'balanced'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e55e505-4c88-42c0-a9b2-edf092468f74",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c38b6-e47f-4200-817d-dc206074d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    batch_size=16,\n",
    "    num_epochs=1,\n",
    "    body_learning_rate = 2e-5, #2e-5,\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    #eval_steps = 250,\n",
    "    #save_strategy=\"steps\",\n",
    "    #save_steps=250,\n",
    "    #load_best_model_at_end=True,\n",
    "    sampling_strategy='oversampling', # default is oversampling. Kinda replace --but still exist, the num_iterations args\n",
    "    max_steps=-1 # default -1 (all). Can also overrides num_epochs and reduce the # steps that would be otherwise needed.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b262dce8-e771-43fc-bef9-f9b0f7510067",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    metric=\"accuracy\",\n",
    "    column_mapping={\"comment\": \"text\", \"label\": \"label\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7cdd3-d8f3-4e21-84b0-764091505c94",
   "metadata": {},
   "source": [
    "#### Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34315995-4be6-47a1-be6e-c524c5d2da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752abd34-209c-40ad-bc6d-62517c86ef00",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d174bcc-88f7-47f7-a14e-98e43cbd34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b56b98-35e4-4423-ae26-c3aa5c5f63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- mpnet\n",
    "# liblinear, max_iter 300, batch sz 16, epoch 1, max_steps = -1 (4k)\n",
    "# result = 65,2%\n",
    "\n",
    "# mpnet\n",
    "# liblinear, max_iter 300, batch sz 16, epoch 1, max_steps = 1000\n",
    "# result = 68%\n",
    "\n",
    "# mpnet\n",
    "# liblinear, max_iter 300, batch sz 16, epoch 1, max_steps = 1500\n",
    "# result = 64 %\n",
    "\n",
    "# ----- dang camembert large\n",
    "# liblinear, max_iter 300, batch sz 2 (37k steps!), epoch 1, max_steps = -1\n",
    "# result = 44,8%\n",
    "\n",
    "# liblinear, max_iter 300, batch sz 3 , epoch 1, max_steps = 1000\n",
    "# result = 59%\n",
    "\n",
    "# ----- dang camembert BASE\n",
    "# liblinear, max_iter 300, batch sz 8, epoch 1, max_steps = 2500, body_learning_rate = 2e-5\n",
    "# result = 65,2%\n",
    "\n",
    "# liblinear, max_iter 200, batch sz 16, epoch 1, max_steps = 1000, body_learning_rate = 2e-5\n",
    "# result = 60%\n",
    "\n",
    "# liblinear, max_iter 300, batch sz 16, epoch 1, max_steps = 3000, body_learning_rate = 2e-5\n",
    "# result = 64,8%\n",
    "\n",
    "# *lbfgs*, max_iter 300, batch sz 16, epoch 1, max_steps = 3000, body_learning_rate = 2e-5\n",
    "# result = 64,8%\n",
    "\n",
    "# *lbfgs*, max_iter 500, batch sz 16, epoch 1, max_steps = 1000, body_learning_rate = 1e-5\n",
    "# result = 64,8%\n",
    "\n",
    "# *lbfgs*, max_iter 300, batch sz 16, epoch 1, max_steps = 1000, body_learning_rate = 1e-5, \"class_weight\": 'balanced'\n",
    "# result = 61,2%\n",
    "\n",
    "# ----- mpnet (again)\n",
    "# *newton-cg*, max_iter 300, batch sz 16, epoch 1, max_steps = 1000, body_learning_rate = 1e-5\n",
    "# result : 63,9%\n",
    "\n",
    "# *liblinear*, max_iter 300, batch sz 16, epoch 1, max_steps = 1000, body_learning_rate = 1e-5\n",
    "# result : 65,2%\n",
    "\n",
    "# *liblinear*, max_iter 300, batch sz 16, epoch 1, max_steps = 750, body_learning_rate = 2e-5\n",
    "# result :65,2 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcb4ba1-170e-4a14-a301-cbc4e14baa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import Trial\n",
    "\n",
    "# Optional, but for test purposes 8 ex. per class\n",
    "train_dataset = sample_dataset(dataset[\"train\"], label_column=\"label\", num_samples=8, seed=40)\n",
    "\n",
    "def model_init(params):\n",
    "    params = params or {}\n",
    "    max_iter = params.get(\"max_iter\", 100)\n",
    "    solver = params.get(\"solver\", \"liblinear\")\n",
    "    params = {\n",
    "        \"head_params\": {\n",
    "            \"max_iter\": max_iter,\n",
    "            \"solver\": solver,\n",
    "        }\n",
    "    }\n",
    "    return SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\", **params)\n",
    "\n",
    "def hp_space(trial):\n",
    "    \"\"\" Define hyperparams search space (Optuna) \"\"\"\n",
    "    \n",
    "    return {\n",
    "        # Embeddings fine-tuning phase params :\n",
    "        \n",
    "        \"body_learning_rate\": trial.suggest_float(\"body_learning_rate\", 1e-6, 1e-3, log=True),\n",
    "        \"num_epochs\": trial.suggest_int(\"num_epochs\", 1, 3),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32]),\n",
    "        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
    "        \n",
    "        # LogisticRegression head params :\n",
    "        \n",
    "        \"max_iter\": trial.suggest_int(\"max_iter\", 50, 300),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"newton-cg\", \"liblinear\",\"lbfgs\"]),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    metric=\"accuracy\",\n",
    "    column_mapping={\"comment\": \"text\", \"label\": \"label\"},\n",
    ")\n",
    "best_run = trainer.hyperparameter_search(direction=\"maximize\", hp_space=hp_space, n_trials=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0566077-f75a-4a15-8547-ee7c268ec13b",
   "metadata": {},
   "source": [
    "See [issue 1](https://github.com/huggingface/setfit/issues/311) and [issue 2](https://github.com/huggingface/transformers/issues/13019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d530cd-9367-4f78-9d4f-3f1c6957d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, but for test purposes 8 ex. per class\n",
    "train_dataset = sample_dataset(dataset[\"train\"], label_column=\"label\", num_samples=70, seed=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb35fbef-5572-4801-b002-0a836f44d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from optuna import Trial\n",
    "from setfit import Trainer, SetFitModel, sample_dataset\n",
    "import time\n",
    "\n",
    "# Model initialization function\n",
    "def model_init(params):\n",
    "    params = params or {}\n",
    "    max_iter = params.get(\"max_iter\", 100)\n",
    "    solver = params.get(\"solver\", \"liblinear\")\n",
    "    params = {\n",
    "        \"head_params\": {\n",
    "            \"max_iter\": max_iter,\n",
    "            \"solver\": solver,\n",
    "        }\n",
    "    }\n",
    "    # memory management\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(2)\n",
    "\n",
    "    return SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\", **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039bce8-ce84-4923-9065-5478896cf43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter space definition\n",
    "def hp_space(trial):\n",
    "    \"\"\" Define hyperparams search space (Optuna) \"\"\"\n",
    "    \n",
    "    return {\n",
    "        # Embeddings fine-tuning phase params :\n",
    "        \n",
    "        \"body_learning_rate\": trial.suggest_float(\"body_learning_rate\", 1e-07 , 3e-06, log=True), #1e-7 , 1e-5 oldest : # 1e-6, 1e-3\n",
    "        # \"num_epochs\": trial.suggest_int(\"num_epochs\", 1, 2),\n",
    "        \"max_steps\": trial.suggest_int(\"max_steps\", 650, 900), # 200, 900\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16]),\n",
    "        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
    "        \n",
    "        # LogisticRegression head params :\n",
    "        \n",
    "        \"max_iter\": trial.suggest_int(\"max_iter\", 120, 130), # 100, 200\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"liblinear\"]), # \"newton-cg\",'lbfgs'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa878fc-c080-4217-8d3e-3df94f7cf826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized run_hp_search_optuna function\n",
    "def run_hp_search_optuna_modified(trainer, n_trials, direction, **kwargs):\n",
    "    import optuna\n",
    "\n",
    "    def _objective(trial):\n",
    "        trainer.objective = None\n",
    "        trainer.train(trial=trial)\n",
    "        \n",
    "        # memory management\n",
    "        del trainer.model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        time.sleep(2)\n",
    "\n",
    "        # Evaluate if needed\n",
    "        if getattr(trainer, \"objective\", None) is None:\n",
    "            metrics = trainer.evaluate()\n",
    "            trainer.objective = trainer.compute_objective(metrics)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        return trainer.objective\n",
    "\n",
    "    timeout = kwargs.pop(\"timeout\", None)\n",
    "    n_jobs = kwargs.pop(\"n_jobs\", 1)\n",
    "    study = optuna.create_study(direction=direction, **kwargs)\n",
    "\n",
    "    # memory management : overkill, but also adding gc_after_trial=True in study.optimize()\n",
    "    study.optimize(_objective, n_trials=n_trials, timeout=timeout, n_jobs=n_jobs, gc_after_trial=True)\n",
    "    best_trial = study.best_trial\n",
    "    return BestRun(str(best_trial.number), best_trial.value, best_trial.params, study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3003054-f911-4989-9db1-e7936a392d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    metric=\"accuracy\",\n",
    "    column_mapping={\"comment\": \"text\", \"label\": \"label\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c5aaf-8df9-46e7-b9a5-7e00fbb6ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the run_hp_search_optuna method with the modified one\n",
    "trainer.run_hp_search_optuna = run_hp_search_optuna_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd90995-d311-498b-9397-70fd42f21344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter search\n",
    "best_run = trainer.hyperparameter_search(direction=\"maximize\", hp_space=hp_space, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a22fae-83ae-4de2-8095-30b7cbdfd718",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1351290b-3df2-4fa4-b32b-7fe8c8f64f49",
   "metadata": {},
   "source": [
    "#### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ba609-2210-4154-9d92-0531d3ae6891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPNET\n",
    "\n",
    "# samples =  70 (total : 1838 examples, max steps range : 200, 900)\n",
    "\n",
    "# Trial 1 finished with value: 0.6347826086956522 and parameters: {'body_learning_rate': 3.303673112561539e-06, 'max_steps': 753, 'batch_size': 16, 'seed': 9, 'max_iter': 183, 'solver': 'liblinear'}\n",
    "# Trial 0 finished with value: ---> 0.6739130434782609 and parameters: {'body_learning_rate': 2.5030305744282964e-06, 'max_steps': 690, 'batch_size': 16, 'seed': 13, 'max_iter': 125, 'solver': 'liblinear'}\n",
    "# and after refinements/runs around trial 0 with 67% \n",
    "# Trial 2 finished with value: ---> 0.6869565217391305 and parameters: {'body_learning_rate': 1.845176533146184e-07, 'max_steps': 653, 'batch_size': 16, 'seed': 16, 'max_iter': 121, 'solver': 'liblinear'}\n",
    "\n",
    "# samples =  NO SAMPLE (total : 2625 examples, max steps range : 650, 800, learning rate 1e-7 , 1e-5, max_iter : 120, 126)\n",
    "\n",
    "# Trial 2 finished with value: 0.6826086956521739 and parameters: {'body_learning_rate': 1.6752640093810652e-06, 'max_steps': 791, 'batch_size': 16, 'seed': 36, 'max_iter': 123, 'solver': 'liblinear'}.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754bfef2-b05d-412f-bdb5-ba55535eaf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, but for test purposes 8 ex. per class\n",
    "train_dataset = sample_dataset(dataset[\"train\"], label_column=\"label\", num_samples=70, seed=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048d6ba-d864-4daa-b9d6-5cfd3a0268b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from optuna import Trial\n",
    "from setfit import Trainer, SetFitModel, sample_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "import time\n",
    "\n",
    "\n",
    "# Model initialization function for RandomForestClassifier\n",
    "def model_init(params):\n",
    "    params = params or {}\n",
    "    max_depth = params.get(\"max_depth\", 2)\n",
    "    n_estimators = params.get(\"n_estimators\", 100)\n",
    "    random_state = params.get(\"random_state\", 0)\n",
    "    params = {\n",
    "        \"head_params\": {\n",
    "            \"max_depth\": max_depth,\n",
    "            \"random_state\": random_state,\n",
    "        }\n",
    "    }\n",
    "    # memory management\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(2)\n",
    "\n",
    "    model_body = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "    model_head = RandomForestClassifier(**params[\"head_params\"])\n",
    "\n",
    "    return SetFitModel(\n",
    "        model_body=model_body,\n",
    "        model_head=model_head,\n",
    "        # multi_target_strategy=None,\n",
    "        # l2_weight=1e-2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c37dc3-0f86-44ad-aaca-ea47920edd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter space definition\n",
    "def hp_space(trial):\n",
    "    \"\"\" Define hyperparams search space (Optuna) \"\"\"\n",
    "    \n",
    "    return {\n",
    "        # Embeddings fine-tuning phase params :\n",
    "        \n",
    "        \"body_learning_rate\": trial.suggest_float(\"body_learning_rate\", 1e-7, 1e-5, log=True),\n",
    "        # \"num_epochs\": trial.suggest_int(\"num_epochs\", 1, 2),\n",
    "        \"max_steps\": trial.suggest_int(\"max_steps\", 650, 900), # 200, 900\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16]),\n",
    "        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
    "        \n",
    "        # RandomForest head params :\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 30),\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1e29d-29c5-4efe-8c8d-e507583ef4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized run_hp_search_optuna function\n",
    "def run_hp_search_optuna_modified(trainer, n_trials, direction, **kwargs):\n",
    "    import optuna\n",
    "\n",
    "    def _objective(trial):\n",
    "        trainer.objective = None\n",
    "        trainer.train(trial=trial)\n",
    "        \n",
    "        # memory management\n",
    "        del trainer.model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "        # Evaluate if needed\n",
    "        if getattr(trainer, \"objective\", None) is None:\n",
    "            metrics = trainer.evaluate()\n",
    "            trainer.objective = trainer.compute_objective(metrics)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        return trainer.objective\n",
    "\n",
    "    timeout = kwargs.pop(\"timeout\", None)\n",
    "    n_jobs = kwargs.pop(\"n_jobs\", 1)\n",
    "    study = optuna.create_study(direction=direction, **kwargs)\n",
    "\n",
    "    # memory management : overkill, but also adding gc_after_trial=True in study.optimize()\n",
    "    study.optimize(_objective, n_trials=n_trials, timeout=timeout, n_jobs=n_jobs, gc_after_trial=True)\n",
    "    best_trial = study.best_trial\n",
    "    return BestRun(str(best_trial.number), best_trial.value, best_trial.params, study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6536265-c2bc-4766-bcc5-1e8236214ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    metric=\"accuracy\",\n",
    "    column_mapping={\"comment\": \"text\", \"label\": \"label\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23984b0-b549-41c6-9a0e-82fb48ec60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the run_hp_search_optuna method with the modified one\n",
    "trainer.run_hp_search_optuna = run_hp_search_optuna_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd45472-c644-4f29-873c-6ebbfeef33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter search\n",
    "best_run = trainer.hyperparameter_search(direction=\"maximize\", hp_space=hp_space, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1add5-4d6f-46bc-a8f8-4de2f74d9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPNET, randomforest\n",
    "\n",
    "# samples =  70 (total : 1838 examples, max steps range : 200, 900\n",
    "# Trial 0 finished with value: 0.6521739130434783 and parameters: {'body_learning_rate': 6.708138590154178e-07, 'max_steps': 705, # 'batch_size': 16, 'seed': 22, 'n_estimators': 325, 'max_depth': 19}.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f01487-f142-45df-91d9-864276d2eb55",
   "metadata": {},
   "source": [
    "Also, some memory management to prevent out of memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e414300-3b2c-45d0-bea3-68571bbe0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, but for test purposes 8 ex. per class\n",
    "train_dataset = sample_dataset(dataset[\"train\"], label_column=\"label\", num_samples=70, seed=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6846318d-f420-44e3-8060-998cca5265b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from optuna import Trial\n",
    "from setfit import Trainer, SetFitModel, sample_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, GradientBoostingClassifier\n",
    "import time\n",
    "\n",
    "\n",
    "# Model initialization function for sklearn \"lightgbm\" (histGrad) or classic GradientBoostingClassifier\n",
    "def model_init(params):\n",
    "    params = params or {}\n",
    "    max_depth = params.get(\"max_depth\", 2)\n",
    "    n_estimators = params.get(\"n_estimators\", 100)\n",
    "    learning_rate = params.get(\"learning_rate\", 0.1)\n",
    "    random_state = params.get(\"random_state\", 0)\n",
    "    params = {\n",
    "        \"head_params\": {\n",
    "            \"max_depth\": max_depth,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"random_state\": random_state,\n",
    "        }\n",
    "    }\n",
    "    # memory management\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(2)\n",
    "\n",
    "    model_body = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "    model_head = GradientBoostingClassifier(**params[\"head_params\"])\n",
    "\n",
    "    return SetFitModel(\n",
    "        model_body=model_body,\n",
    "        model_head=model_head,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735941f3-fdb8-44ae-98a8-80604ef96538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter space definition\n",
    "def hp_space(trial):\n",
    "    \"\"\" Define hyperparams search space (Optuna) \"\"\"\n",
    "    \n",
    "    return {\n",
    "        # Embeddings fine-tuning phase params :\n",
    "        \n",
    "        #\"body_learning_rate\": trial.suggest_float(\"body_learning_rate\", 1e-7, 1e-5, log=True),\n",
    "        # \"num_epochs\": trial.suggest_int(\"num_epochs\", 1, 2),\n",
    "        \"max_steps\": trial.suggest_int(\"max_steps\", 650, 1100), # 200, 900\n",
    "        #\"batch_size\": trial.suggest_categorical(\"batch_size\", [16]),\n",
    "        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
    "        \n",
    "        # classifier head params :\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.01, 0.1, 0.2]),\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758d311-bc53-42ec-9520-6afb5e17db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized run_hp_search_optuna function\n",
    "def run_hp_search_optuna_modified(trainer, n_trials, direction, **kwargs):\n",
    "    import optuna\n",
    "\n",
    "    def _objective(trial):\n",
    "        trainer.objective = None\n",
    "        trainer.train(trial=trial)\n",
    "        \n",
    "        # memory management\n",
    "        del trainer.model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "        # Evaluate if needed\n",
    "        if getattr(trainer, \"objective\", None) is None:\n",
    "            metrics = trainer.evaluate()\n",
    "            trainer.objective = trainer.compute_objective(metrics)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        return trainer.objective\n",
    "\n",
    "    timeout = kwargs.pop(\"timeout\", None)\n",
    "    n_jobs = kwargs.pop(\"n_jobs\", 1)\n",
    "    study = optuna.create_study(direction=direction, **kwargs)\n",
    "\n",
    "    # memory management : overkill, but also adding gc_after_trial=True in study.optimize()\n",
    "    study.optimize(_objective, n_trials=n_trials, timeout=timeout, n_jobs=n_jobs, gc_after_trial=True)\n",
    "    best_trial = study.best_trial\n",
    "    return BestRun(str(best_trial.number), best_trial.value, best_trial.params, study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2843cfb1-01cd-47ad-b5e1-5dea001761d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    batch_size=16,\n",
    "    num_epochs=1,\n",
    "    body_learning_rate = 1.845176533146184e-07,\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    #eval_steps = 250,\n",
    "    #save_strategy=\"steps\",\n",
    "    #save_steps=250,\n",
    "    #load_best_model_at_end=True,\n",
    "    sampling_strategy='oversampling', # default is oversampling. Kinda replace --but still exist, the num_iterations args\n",
    "    # max_steps=-1 # default -1 (all). Can also overrides num_epochs and reduce the # steps that would be otherwise needed.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5555d855-ba6b-419d-aea3-a96d65f90c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    metric=\"accuracy\",\n",
    "    column_mapping={\"comment\": \"text\", \"label\": \"label\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638acc9-9626-4e6c-9711-74734c2559ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the run_hp_search_optuna method with the modified one\n",
    "trainer.run_hp_search_optuna = run_hp_search_optuna_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0507bc-6bab-4ce2-b735-a880ea6fcdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter search\n",
    "best_run = trainer.hyperparameter_search(direction=\"maximize\", hp_space=hp_space, n_trials=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
