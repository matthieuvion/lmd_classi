{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## lmd - Models evaluation SetFit (baseline) and fine-tuned mistral","metadata":{"_uuid":"6e9398f8-f5aa-4432-9cbb-2f2a50a31843","_cell_guid":"92d0b43b-d3ef-45a2-8592-27c3077a650e","trusted":true}},{"cell_type":"markdown","source":"**Steps / end goal**\n1. Started with our +-500 human annotated comments (out of 200k)\n2. Synthetic data generation (comments + label) w/ Mistral OpenHermes : around 2k samples\n3. Prepare instruction dataset, before fine tuning, using Alpaca format  \n4. Fine-tune mistral-7B (classif. / label completion), using unsloth, on train + synthetic data.  \n5. More tests on the fine-tuned model. If good enough, labels unlabeled data to several k examples (fine-tuned model as a classifier or weighted avg. w/ our Few shot SetFit baseline). **<- we're here**\n6. Extend dataset to several 20k examples with fine-tuned Mistral (and/or ensemble model w/ Setfit) doing the classification.  \n7. End goal being deployment/inference performance: train a classifier on the extended dataset using bge-m3 or multi-e5 embeddings. ","metadata":{"_uuid":"19e388c8-994e-4433-9875-cd321fd66cdd","_cell_guid":"32b81653-4e14-47d3-a0de-4411deb90ca9","trusted":true}},{"cell_type":"markdown","source":"**Ressources**  \n- [MLabonne Repo](https://github.com/mlabonne/llm-course)  \n- [Dataset Gen - Kaggle example](https://www.kaggle.com/code/phanisrikanth/generate-synthetic-essays-with-mistral-7b-instruct)  \n- [Dataset Gen - blog w/ prompt examples](https://hendrik.works/blog/leveraging-underrepresented-data)  \n- [Prepare dataset- /r/LocalLLaMA best practice classi](https://www.reddit.com/r/LocalLLaMA/comments/173o5dv/comment/k448ye1/?utm_source=reddit&utm_medium=web2x&context=3)  \n- [Prepare dataset - using gpt3.5](https://medium.com/@kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f) \n- [Prepare dataset - Predibase prompts for diverse fine-tuning tasks](https://predibase.com/lora-land)\n- [Fine tune OpenHermes-2.5-Mistral-7B - including prompt template gen](https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac)  \n- [Fine tune - Unsloth colab example](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)\n- [Fine tune - w/o unsloth](https://gathnex.medium.com/mistral-7b-fine-tuning-a-step-by-step-guide-52122cdbeca8) or [wandb](https://wandb.ai/vincenttu/finetuning_mistral7b/reports/Fine-tuning-Mistral-7B-with-W-B--Vmlldzo1NTc3MjMy) or [philschmid](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl#6-deploy-the-llm-for-production)\n- [Fine tune - impact of parameters S. Raschka](https://lightning.ai/pages/community/lora-insights/)","metadata":{"_uuid":"2328ae63-ba6f-4dd3-ab6a-e3f4f2ecd0f9","_cell_guid":"c3e669b4-c086-4a2e-b574-612aab27c460","trusted":true}},{"cell_type":"code","source":"%%capture\n# take several minutes, uncomment %%capture to see installation details\n!mamba install -q cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=11.8 \\\n                   -c pytorch -c nvidia -c xformers -c conda-forge -y\n!pip install \"unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git\"\n!pip uninstall -q datasets -y\n!pip install -q datasets\n\n!pip install \"git+https://github.com/huggingface/transformers.git\"","metadata":{"_uuid":"ba792832-f6db-44bb-adf6-9a6f54820b59","_cell_guid":"005bbad3-5755-44b1-b4d4-2a74d76bec5c","collapsed":false,"scrolled":true,"execution":{"iopub.status.busy":"2024-02-29T10:08:39.169467Z","iopub.execute_input":"2024-02-29T10:08:39.169816Z","iopub.status.idle":"2024-02-29T10:12:09.644659Z","shell.execute_reply.started":"2024-02-29T10:08:39.169785Z","shell.execute_reply":"2024-02-29T10:12:09.643315Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# should not be required, but had to, as of 2024 feb 28th\n!pip install -q bitsandbytes triton xformers","metadata":{"_uuid":"dd5fd415-fa02-435f-87ef-f4342a5c481d","_cell_guid":"3cdca4c7-0b9b-4841-99b7-25fc8212f63f","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:12:09.646802Z","iopub.execute_input":"2024-02-29T10:12:09.647128Z","iopub.status.idle":"2024-02-29T10:14:34.435832Z","shell.execute_reply.started":"2024-02-29T10:12:09.647097Z","shell.execute_reply":"2024-02-29T10:14:34.434473Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# install setfit (few shot classifier) and evaluate from HuggingFace\n!pip install -q setfit evaluate","metadata":{"_uuid":"8a6fe9cf-9371-4055-8ca6-3c34f341a603","_cell_guid":"3fe7e94a-0bb4-4b20-9ba8-37a4dec254f1","collapsed":false,"scrolled":true,"execution":{"iopub.status.busy":"2024-02-29T10:14:34.437346Z","iopub.execute_input":"2024-02-29T10:14:34.437655Z","iopub.status.idle":"2024-02-29T10:14:48.632582Z","shell.execute_reply.started":"2024-02-29T10:14:34.437626Z","shell.execute_reply":"2024-02-29T10:14:48.631460Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport re\nimport numpy as np\nimport pandas as pd\nfrom time import perf_counter\n\nfrom unsloth import FastLanguageModel\nfrom setfit import SetFitModel\n\nfrom datasets import load_dataset, Dataset, DatasetDict\nimport evaluate\nfrom evaluate import load\nfrom evaluate.visualization import radar_plot\nfrom sklearn.metrics import accuracy_score, classification_report","metadata":{"_uuid":"61874892-9cab-4cc9-bca7-7a438e3267ac","_cell_guid":"86b406cc-e29f-4714-8ecb-45fc29b9fe44","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:44:39.335457Z","iopub.execute_input":"2024-02-29T10:44:39.335908Z","iopub.status.idle":"2024-02-29T10:44:39.342315Z","shell.execute_reply.started":"2024-02-29T10:44:39.335876Z","shell.execute_reply":"2024-02-29T10:44:39.341165Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nDEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f\"Device: {DEVICE}\")\nprint(f\"CUDA Version: {torch.version.cuda}\")\nprint(f\"Pytorch {torch.__version__}\")","metadata":{"_uuid":"6e0d87a0-96b1-4b43-a096-b575d335a4c4","_cell_guid":"0f02dd46-9647-466c-bc7d-84356ba76e08","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:19:20.868235Z","iopub.execute_input":"2024-02-29T10:19:20.869073Z","iopub.status.idle":"2024-02-29T10:19:20.874874Z","shell.execute_reply.started":"2024-02-29T10:19:20.869044Z","shell.execute_reply":"2024-02-29T10:19:20.873743Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load datasets : original data (train/test, manually labeled)","metadata":{"_uuid":"1beb2024-89e6-4048-b795-4349c67656ad","_cell_guid":"3aac9107-8d84-415b-8104-1c6843e863b7","trusted":true}},{"cell_type":"code","source":"# load original, custom and human-annotated dataset, previously saved on HF\nfilepath = \"gentilrenard/lmd_ukraine_comments\"\n\n# HF Datasets format\nds = load_dataset(filepath)","metadata":{"_uuid":"6594f5d8-5866-4b41-88c9-a7089f352b4d","_cell_guid":"d1ae6fda-a6f6-4efa-9e97-fcda7f721fbf","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:19:20.876200Z","iopub.execute_input":"2024-02-29T10:19:20.876575Z","iopub.status.idle":"2024-02-29T10:19:31.827602Z","shell.execute_reply.started":"2024-02-29T10:19:20.876542Z","shell.execute_reply":"2024-02-29T10:19:31.826534Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract train and eval datasets from DatasetDict\ntrain_dataset = ds['train']\neval_dataset = ds['validation']\n\n# Define our eval column with ground truth labels\neval_labels = eval_dataset[\"label\"]\n\n# dataset structure\nprint(ds)","metadata":{"_uuid":"45f58dd8-bea4-41c8-9c6c-6e57b55ded8d","_cell_guid":"6004993f-f07a-4ad1-a1b9-7aeea13cc98e","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:19:31.829483Z","iopub.execute_input":"2024-02-29T10:19:31.829825Z","iopub.status.idle":"2024-02-29T10:19:31.834614Z","shell.execute_reply.started":"2024-02-29T10:19:31.829799Z","shell.execute_reply":"2024-02-29T10:19:31.833734Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Evaluation metrics / func","metadata":{}},{"cell_type":"code","source":"def perf_global(predictions: list[int], references: list[int]) -> dict[str, float]:\n    \"\"\"\n    Computes model global perf. metrics.\n    \n    Args:\n    predictions (list[int]): The predicted labels by the model.\n    references (list[int]): The true labels.\n    \"\"\"\n    # Load metrics from evaluate\n    accuracy_metric = load(\"accuracy\")\n    f1_metric = load(\"f1\", config_name=\"multiclass\")\n    precision_metric = load(\"precision\", config_name=\"multiclass\")\n    recall_metric = load(\"recall\", config_name=\"multiclass\")\n    \n    accuracy_result = accuracy_metric.compute(predictions=predictions, references=references)\n    f1_result = f1_metric.compute(predictions=predictions, references=references, average=\"macro\")\n    precision_result = precision_metric.compute(predictions=predictions, references=references, average=\"macro\")\n    recall_result = recall_metric.compute(predictions=predictions, references=references, average=\"macro\")\n    \n    return {\n        \"accuracy\": accuracy_result[\"accuracy\"],\n        \"f1\": f1_result[\"f1\"],\n        \"precision\": precision_result[\"precision\"],\n        \"recall\": recall_result[\"recall\"]\n    }\n\n\ndef perf_per_class(predictions: list[int], references: list[int]) -> dict[str, float]:\n    \"\"\"\n    Compute metrics per class.\n    \"\"\"\n    f1_detail = load(\"f1\", config_name=\"multiclass\", average=None)\n    precision_detail = load(\"precision\", config_name=\"multiclass\", average=None)\n    recall_detail = load(\"recall\", config_name=\"multiclass\", average=None)\n    \n    f1_result = f1_detail.compute(predictions=predictions, references=references, average=None)\n    precision_result = precision_detail.compute(predictions=predictions, references=references, average=None)\n    recall_result = recall_detail.compute(predictions=predictions, references=references, average=None)\n    \n    return {\n        \"f1_per_class\": f1_result[\"f1\"],\n        \"precision_per_class\": precision_result[\"precision\"],\n        \"recall_per_class\": recall_result[\"recall\"],\n    }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Eval SetFit model (baseline)","metadata":{"_uuid":"99346599-d6cd-486d-b831-bd01156a1d3c","_cell_guid":"68c6973f-cc32-403b-aa99-dad9c91eb894","trusted":true}},{"cell_type":"markdown","source":"Our SetFit model was trained upon `paraphrase-multilingual-mpnet-base-v2` with a logistic head and optimized hyperparameters. Model is saved on [HF hub](https://huggingface.co/gentilrenard/paraphrase-multilingual-mpnet-base-v2_setfit-lemonde-french).","metadata":{"_uuid":"cc64d11f-f79a-46a1-bce9-005e57b51931","_cell_guid":"46448d87-5162-4fce-a6be-8b1cc7e48557","trusted":true}},{"cell_type":"code","source":"# Download Setfit model (incl. logistic head) from Hub\nfilepath_model = \"gentilrenard/paraphrase-multilingual-mpnet-base-v2_setfit-lemonde-french\"\nsetfit_model = SetFitModel.from_pretrained(filepath_model)","metadata":{"_uuid":"87c29189-7dad-413b-b292-ee72f79d1f6e","_cell_guid":"a5956f7a-68f9-4791-a60d-097b1800f6b1","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:23:00.305359Z","iopub.execute_input":"2024-02-29T10:23:00.305769Z","iopub.status.idle":"2024-02-29T10:23:10.984913Z","shell.execute_reply.started":"2024-02-29T10:23:00.305738Z","shell.execute_reply":"2024-02-29T10:23:10.984135Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict","metadata":{"_uuid":"eb68b031-11c7-4913-b4fb-e83e09ad765f","_cell_guid":"a8ce4d60-11b9-4483-b6f1-8dc5ea152c9a","trusted":true}},{"cell_type":"markdown","source":"Predict on a few (fake, simple) samples. 0 is pro_ukraine, 1: pro_russia, 2: off topic/don't know","metadata":{"_uuid":"dc82b9ef-e3f5-4bed-acc4-ad530a63bb26","_cell_guid":"b06eca1e-b4ea-4015-b9d7-f4596dc92d5e","trusted":true}},{"cell_type":"code","source":"# Run inference\npreds = setfit_model.predict(\n    [\n        \"La Russie va gagner cette guerre, ils ont plus de ressources\",\n        \"les journalistes sont corrompus, le traitement est partial\",\n        \"les pauvres ukrainiens se font anéantir et subissent des crimes de guerre\",\n        \"La France doit donner plus d'armes à l'ukraine\"\n    ]\n)\nprint(preds)","metadata":{"_uuid":"13fd982e-11ce-4a10-b9d4-79439a5814eb","_cell_guid":"8d65b63a-9ca5-4bfa-96f6-41541ccf6553","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:23:20.901738Z","iopub.execute_input":"2024-02-29T10:23:20.902417Z","iopub.status.idle":"2024-02-29T10:23:21.436548Z","shell.execute_reply.started":"2024-02-29T10:23:20.902383Z","shell.execute_reply":"2024-02-29T10:23:21.435606Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predict on full evaluation dataset.  \nWe're adding a perf counter to compute latency. Not the best implementation here but will give us a rough idea.","metadata":{"_uuid":"2579d2a1-7319-499c-9278-54f93eb0852d","_cell_guid":"5de9d91d-2186-43a7-a43c-496364efe6eb","trusted":true}},{"cell_type":"code","source":"eval_samples = eval_dataset[\"text\"]\nstart_time = perf_counter()\n\nsetfit_preds = setfit_model.predict(eval_samples, batch_size=32, as_numpy=False, use_labels=False)\n\nsetfit_latency = perf_counter() - start_time\nsetfit_avg_latency = 1000 * (setfit_latency/139)\nprint(f\"setfit_avg_latency (gpu): {setfit_avg_latency}\")","metadata":{"_uuid":"2854545b-0dc7-442f-b57b-3c62b46274e3","_cell_guid":"9e18eef4-f115-4f51-86bf-1eb23cc58017","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:23:24.933184Z","iopub.execute_input":"2024-02-29T10:23:24.934155Z","iopub.status.idle":"2024-02-29T10:23:25.663188Z","shell.execute_reply.started":"2024-02-29T10:23:24.934119Z","shell.execute_reply":"2024-02-29T10:23:25.662165Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate","metadata":{"_uuid":"ea9f377e-09f0-48e6-a97b-e527e60609e4","_cell_guid":"5bfcfd4b-845d-4ddc-9015-d65cd1fc37bc","trusted":true}},{"cell_type":"markdown","source":"TL;DR  \nOverall SetFit provided good results (76% accuracy), with a few shots approach, on a difficult, real life dataset ;). Train set have around 100 labels per class only.  \nSetfit model performs best on Class 2 (identifying off topic/no clear opinion to the conflict), with excellent Precision and Recall, indicated by a high F1 Score.  \nClass 0 (pro Ukraine) has good results, with balanced Precision and Recall.  \nClass 1 (pro Russia), room for improvement : the model struggles to identify all actual instances of pro_russian comment.  \nTo be honest, it was even hard for me (the annotator) to find a lot and clear pro russia comments. Also, several days after, I find myself questionning whether some of the label could be labeled otherwise.","metadata":{"_uuid":"c6fb1854-9c08-414e-adce-1c14e6bd678f","_cell_guid":"d4293d30-8fe6-4bbc-ba9c-d38a6205e74f","trusted":true}},{"cell_type":"code","source":"# overall metrics\nsetfit_global_metrics = perf_global(predictions=setfit_preds, references=eval_labels)\nprint(f\"Model overall performance:\\n{setfit_global_metrics}\")\n\n# detail per class\nsetfit_detailed_metrics = perf_per_class(predictions=setfit_preds, references=eval_labels)\n\nprint(f\"Per class:\\n{setfit_detailed_metrics}\")","metadata":{"_uuid":"3ad3b9f3-4d5b-488e-88ee-7491eee1515d","_cell_guid":"458dc50a-4b27-4253-b858-497f5284d1e1","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:24:56.255776Z","iopub.execute_input":"2024-02-29T10:24:56.256689Z","iopub.status.idle":"2024-02-29T10:24:59.325526Z","shell.execute_reply.started":"2024-02-29T10:24:56.256656Z","shell.execute_reply":"2024-02-29T10:24:59.324549Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Eval data-augmented, fine-tuned mistral","metadata":{"_uuid":"f188f31c-d5d4-43a9-9d7b-717ec4f38444","_cell_guid":"55efded8-7e5f-4d1b-8c90-8920f4a30f24","trusted":true}},{"cell_type":"markdown","source":"Our (4b quantized) Mistral-7B was fine-tuned using `Unsloth` on labeled data + synthetic data generated by Mistral-7B OpenHermes variant. LoRa adapters : alpha=16 [HF hub](https://huggingface.co/gentilrenard/Mistral-7B-lora-lmd-en), alpha=8 [HF hub](https://huggingface.co/gentilrenard/Mistral-7B-lora-lmd-en-v2)","metadata":{"_uuid":"a1de03ed-dffd-4728-9bcc-8473f9410c01","_cell_guid":"33c8e142-d78f-4cfb-b318-0e79b676185a","trusted":true}},{"cell_type":"code","source":"lora_hf_filename = \"gentilrenard/Mistral-7B-lora-lmd-en-v3\" # mistral 0.2 alpha=16\n# lora_hf_filename = \"gentilrenard/Mistral-7B-lora-lmd-en\" # mistral 0.1 alpha=16\n# lora_hf_filename = \"gentilrenard/Mistral-7B-lora-lmd-en-v2\" # mistral 0.1 alpha=8","metadata":{"_uuid":"013af2ee-4ae2-40d0-8026-6175b1a625f6","_cell_guid":"cddb7b39-55a0-4c03-9432-21868da1616b","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:27:23.724646Z","iopub.execute_input":"2024-02-29T10:27:23.725072Z","iopub.status.idle":"2024-02-29T10:27:23.729571Z","shell.execute_reply.started":"2024-02-29T10:27:23.725041Z","shell.execute_reply":"2024-02-29T10:27:23.728603Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load saved LoRa adapters from HuggingFace\n# unsloth automatically patches a 4bit quantized mistral\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = lora_hf_filename,\n    max_seq_length = 2048,\n    dtype = None, # autodetection by unsloth\n    load_in_4bit = True,\n)\n\nFastLanguageModel.for_inference(model)","metadata":{"_uuid":"79df13b9-26ca-411d-896d-d5e973df4007","_cell_guid":"405bd35b-2b24-41e5-986e-a1f501fa1185","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:31:26.395361Z","iopub.execute_input":"2024-02-29T10:31:26.396017Z","iopub.status.idle":"2024-02-29T10:32:00.206334Z","shell.execute_reply.started":"2024-02-29T10:31:26.395983Z","shell.execute_reply":"2024-02-29T10:32:00.205310Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict","metadata":{"_uuid":"d99317e1-3edc-4ca1-b270-36b01740bc5f","_cell_guid":"4db819eb-f8da-416e-8bbb-42ce09aa6daa","trusted":true}},{"cell_type":"markdown","source":"This our crafted prompt, used for fine-tuning.","metadata":{"_uuid":"b013ec47-668c-4d68-925d-2d3b196bbf0e","_cell_guid":"c9a1fd0f-8b5e-47d7-9739-4891fafb83d2","trusted":true}},{"cell_type":"code","source":"prompt = \"\"\"You are a helpful, precise, detailed, and concise artificial intelligence assistant with a deep expertise in reading and interpreting comments about Ukraine invasion by Russia.\nIn February 2022, Russia's invasion of Ukraine escalated the ongoing conflict in Ukraine Dombass region since 2014 and is causing massive casualties. President Putin claimed the operation aimed to \"demilitarize and denazify\" Ukraine. Despite Russian territorial gains, Ukraine's resistance and counterattacks have reclaimed some areas. The international community responded with sanctions, support for Ukraine, and legal actions against Russia.\nYou are very intelligent and sharp, having a keen ability and nuanced enough to distinguish which side of the conflict the comment is on.\nYour task is to classify a comment into one of 3 labels : 0, 1 or 2. Possible labels and their meaning:\n- 0: rather in favor of Ukraine and its allies. Support sanctions against Russia or criticizes Russian policy. Ukraine will win.\n- 1: in favor of Russia, even if only so slightly. Criticizes Ukraine, Western, UE or OTAN policies against Russia. Fears of a costly escalation if support is brought to Ukraine. Russia will win.\n- 2: irrelevant to the conflict or does not take side.\nYou will be evaluated based on the following criteria: - The generated answer is best matching label for the comment. - The generated answer is always one label (0, 1 or 2).\nCategorize the comment into a single comment label only:\n### Comment:\n{}\n### Comment label:\n{}\"\"\"","metadata":{"_uuid":"1fd57f9a-f34a-45c7-a5e4-4a671e7e8f84","_cell_guid":"a48654ca-5c60-4da2-9227-dfdda2b5e41c","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:39:11.346063Z","iopub.execute_input":"2024-02-29T10:39:11.346899Z","iopub.status.idle":"2024-02-29T10:39:11.352510Z","shell.execute_reply.started":"2024-02-29T10:39:11.346865Z","shell.execute_reply":"2024-02-29T10:39:11.351566Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predict one sample","metadata":{"_uuid":"ca2270c1-1074-4c0a-a113-d280d34ad962","_cell_guid":"31e7689b-d9a1-4a8c-8949-f3bdfdd03352","trusted":true}},{"cell_type":"code","source":"comment = eval_dataset[22]['text']\nprint(f\"Comment + label :\\n{eval_dataset[22]}\")\nprint(f\"Comment :\\n{comment}\")","metadata":{"_uuid":"f172920a-063b-4a49-82bc-71247ffc9d3b","_cell_guid":"68b9ead0-af9f-4f63-b9d3-ac06aad7308c","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:42:40.824203Z","iopub.execute_input":"2024-02-29T10:42:40.824603Z","iopub.status.idle":"2024-02-29T10:42:40.830824Z","shell.execute_reply.started":"2024-02-29T10:42:40.824575Z","shell.execute_reply":"2024-02-29T10:42:40.829824Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = tokenizer(\n    [\n        prompt.format(\n        comment, # insert comment\n        \"\", # output - blank (instead of label) for generation\n        )\n    ],\n    return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 10, use_cache = True)\ntokenizer.batch_decode(outputs)","metadata":{"_uuid":"ea15fa88-da63-462d-aadf-84599a1c1705","_cell_guid":"46c0308b-66e2-4d73-94e1-252ba4ca8829","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:42:55.381389Z","iopub.execute_input":"2024-02-29T10:42:55.382157Z","iopub.status.idle":"2024-02-29T10:43:02.903201Z","shell.execute_reply.started":"2024-02-29T10:42:55.382126Z","shell.execute_reply":"2024-02-29T10:43:02.902267Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_label(decoded_outputs):\n    \"\"\"Simple parser to extract label from LLM decoded output\"\"\"\n    matches = re.findall(r'Comment label:\\n(\\d)', decoded_outputs[0])\n    return int(matches[-1]) if matches else decoded_outputs","metadata":{"_uuid":"573952c8-ffab-4d5c-bbfb-f91eab105256","_cell_guid":"161b1764-eae1-4089-85d1-e3acc3e6061f","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:52:44.740962Z","iopub.execute_input":"2024-02-29T10:52:44.741555Z","iopub.status.idle":"2024-02-29T10:52:44.747367Z","shell.execute_reply.started":"2024-02-29T10:52:44.741519Z","shell.execute_reply":"2024-02-29T10:52:44.746112Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predict full evaluation dataset and store parsed predicted labels.  \nWe're adding a perf_counter at runtime to measure model latency","metadata":{"_uuid":"291c7aea-6548-4945-8701-fcf7b6d3cc51","_cell_guid":"4585c196-458c-4ebf-a547-c9b0922933f8","trusted":true}},{"cell_type":"code","source":"mistral_preds = []\nmistral_latencies = []\n\nfor i, sample in enumerate(eval_dataset):\n    start_time = perf_counter()\n    inputs = tokenizer(\n        [\n            prompt.format(\n            sample['text'], # insert comment\n            \"\", # output - blank (instead of label) for generation\n            )\n        ],\n        return_tensors = \"pt\").to(\"cuda\")\n\n    outputs = model.generate(**inputs, max_new_tokens = 20, use_cache = True)\n    decoded_outputs = tokenizer.batch_decode(outputs)\n    \n    # parse label from LLM answer\n    predicted_label = parse_label(decoded_outputs)\n    \n    # store results\n    mistral_preds.append(predicted_label)\n    \n    # latency measurement\n    mistral_latency = perf_counter() - start_time\n    mistral_latencies.append(mistral_latency)\n    \n    # look at what goes wrong/well\n    #print(f\"Comment {i}:\\n{sample['text']}\\nTrue label: {sample['label']}\\nPredicted label:\\n{predicted_label}\")\n    \n# Compute run statistics\ntime_avg_ms = 1000 * np.mean(mistral_latencies)\ntime_std_ms = 1000 * np.std(mistral_latencies)","metadata":{"_uuid":"6d27b3c9-d050-4f5e-96dc-e8a0d371f4fb","_cell_guid":"87a115ad-9bdd-4615-a52a-14c327a8d6ef","collapsed":false,"execution":{"iopub.status.busy":"2024-02-29T10:53:08.715582Z","iopub.execute_input":"2024-02-29T10:53:08.716229Z","iopub.status.idle":"2024-02-29T11:00:58.480663Z","shell.execute_reply.started":"2024-02-29T10:53:08.716196Z","shell.execute_reply":"2024-02-29T11:00:58.479750Z"},"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate","metadata":{"_uuid":"c0cea3a8-74d1-4a65-b259-356ba77dcfa5","_cell_guid":"5e926921-b93e-47cd-8243-f33a8ac4aa1b","trusted":true}},{"cell_type":"code","source":"mistral_global_metrics = perf_global(predictions=mistral_preds, references=eval_labels)\nprint(f\"Model overall performance:\\n{mistral_global_metrics}\")\n\nmistral_detailed_metrics = perf_per_class(predictions=mistral_preds, references=eval_labels)\nprint(f\"Per class:\\n{mistral_detailed_metrics}\")\n\nmistral_latency_metrics = {\"avg latency\":time_avg_ms, \"std latency\":time_std_ms}\nprint(mistral_latency_metrics)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:20:23.888083Z","iopub.execute_input":"2024-02-29T11:20:23.889144Z","iopub.status.idle":"2024-02-29T11:20:23.931496Z","shell.execute_reply.started":"2024-02-29T11:20:23.889106Z","shell.execute_reply":"2024-02-29T11:20:23.930563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models benchmark","metadata":{}},{"cell_type":"markdown","source":"Our end goal is to enrich our non-synthethic dataset (=unlabeled data and not synthetic) with model(s) predictions to fine-tune a Bert-like model with more than 300 original samples. We're interested in low cost inference / deployment.\n- Setfit shines on identifying off topic (see above). Quite weak for prediction class 1 (pro russian)\n- We will value precision a bit more than recall, to be sure to retain more \"precise\" examples, over quantity.\n- Our class 1 (pro_russian comments) is the minority class, but the most important for us, re. qualitative objectives.","metadata":{}},{"cell_type":"markdown","source":"**Models comparison : overall performance**","metadata":{}},{"cell_type":"code","source":"model_perf = [\n    {\"Accuracy\":0.76, \"F1\":0.73, \"Precision\": 0.74, \"Recall\":0.73},\n    {\"Accuracy\":0.81, \"F1\":0.80, \"Precision\":0.83, \"Recall\":0.79},\n    {\"Accuracy\":0.79, \"F1\":0.78, \"Precision\":0.79, \"Recall\":0.79},\n    {\"Accuracy\":0.80, \"F1\":0.80, \"Precision\":0.80, \"Recall\":0.80},\n   ]\nmodel_names = [\"Setfit\", \"Mistral_ft_α=16\", \"Mistral_ft_α=8\", \"Mistral_0_2_ft_α=16\"]\nplot = radar_plot(data=model_perf, model_names=model_names)\nplot.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T15:03:24.099993Z","iopub.execute_input":"2024-02-29T15:03:24.100954Z","iopub.status.idle":"2024-02-29T15:03:24.440653Z","shell.execute_reply.started":"2024-02-29T15:03:24.100910Z","shell.execute_reply":"2024-02-29T15:03:24.439373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Per class : class 0 (pro_ukrain)**","metadata":{}},{"cell_type":"code","source":"cls_0_perf = [\n    {\"F1\":0.71, \"Precision\":0.70, \"Recall\":0.72},\n    {\"F1\":0.79, \"Precision\":0.72, \"Recall\":0.88},\n    {\"F1\":0.77, \"Precision\":0.74, \"Recall\":0.8},\n    {\"F1\":0.79, \"Precision\":0.79, \"Recall\":0.78},\n   ]\nmodel_names = [\"Setfit\", \"Mistral_ft_α=16\", \"Mistral_ft_α=8\", \"Mistral_0_2_ft_α=16\"]\nplot = radar_plot(data=cls_0_perf, model_names=model_names)\nplot.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Per class : class 1 (pro_russian)**","metadata":{}},{"cell_type":"code","source":"cls_1_perf = [\n    {\"F1\":0.57, \"Precision\":0.64, \"Recall\":0.51},\n    {\"F1\":0.74, \"Precision\":0.92, \"Recall\":0.63},\n    {\"F1\":0.72, \"Precision\":0.75, \"Recall\":0.68},\n    {\"F1\":0.75, \"Precision\":0.76, \"Recall\":0.74},\n   ]\nmodel_names = [\"Setfit\", \"Mistral_ft_α=16\", \"Mistral_ft_α=8\", \"Mistral_0_2_ft_α=16\"]\nplot = radar_plot(data=cls_1_perf, model_names=model_names)\nplot.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Per class : class 2 (off_topic)**","metadata":{}},{"cell_type":"code","source":"cls_2_perf = [\n    {\"F1\":0.91, \"Precision\":0.87, \"Recall\":0.96},\n    {\"F1\":0.87, \"Precision\":0.87, \"Recall\":0.87},\n    {\"F1\":0.85, \"Precision\":0.86, \"Recall\":0.85},\n    {\"F1\":0.85, \"Precision\":0.84, \"Recall\":0.87},\n   ]\nmodel_names = [\"Setfit\", \"Mistral_ft_α=16\", \"Mistral_ft_α=8\", \"Mistral_0_2_ft_α=16\"]\nplot = radar_plot(data=cls_2_perf, model_names=model_names)\nplot.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Voting ensemble","metadata":{}},{"cell_type":"markdown","source":"In lack of output probabilities, simple mechanism taking advantage of SetFit perf on detecting class 2 and Fine-tuned Mistral on synthetically augmented data for label 0 or 1","metadata":{}},{"cell_type":"code","source":"# We simulate on eval dataset. Could save time at inference w/ skipping mistral predict if Setfit label == 2\nvoted_preds = []\nfor setfit_pred, mistral_pred in zip(setfit_preds, mistral_preds):\n    if setfit_pred == 2:\n        voted_preds.append(setfit_pred)\n    else:\n        voted_preds.append(mistral_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble_global_metrics = perf_global(predictions=voted_preds, references=eval_labels)\nprint(f\"Model overall performance:\\n{ensemble_global_metrics}\")\n\nensemble_detailed_metrics = perf_per_class(predictions=voted_preds, references=eval_labels)\nprint(f\"Per class:\\n{ensemble_detailed_metrics}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Models comparison : ft-mistral vs. basic voting ensemble (ft-mistral + Setfit for class 2)  \nNot too convinced, at least with this basic approach. Precision gain on class of interest (1) is good tho.  \nAdvantage of voting ensemble will be in label prediction : setfit will discriminate in 10ms between class 2 vs (0 or 1) with very good accuracy, and ft Mistral will predict 0 or 1 (900ms latency).","metadata":{}},{"cell_type":"code","source":"{\"F1\":0.75, \"Precision\":0.76, \"Recall\":0.74},","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Model overall performance:\n{'accuracy': 0.8129496402877698, 'f1': 0.7989719700477534, 'precision': 0.8217062833978256, 'recall': 0.7928747795414463}\nPer class:\n{'f1_per_class': array([0.77894737, 0.74193548, 0.87603306]), 'precision_per_class': array([0.82222222, 0.85185185, 0.79104478]), 'recall_per_class': array([0.74      , 0.65714286, 0.98148148])}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bench = [\n    {\"Accuracy\":0.80, \"F1\":0.80, \"Precision\":0.80, \"Recall\":0.80},\n    {\"Accuracy\":0.81, \"F1\":0.80, \"Precision\":0.82, \"Recall\":0.79},\n]\nmodel_names = [\"Mistral_0_2_ft_α=16\", \"Voting ensemble\"]\nplot = radar_plot(data=bench, model_names=model_names)\nplot.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# focus class of interest 'pro russian' (1)\nbench_cls_1 = [\n    {\"F1\":0.75, \"Precision\":0.76, \"Recall\":0.74},\n    {\"F1\":0.74, \"Precision\":0.85, \"Recall\":0.65},\n   ]\nmodel_names = [\"Mistral_0_2_ft_α=16\", \"Voting ensemble\"]\nplot = radar_plot(data=bench_cls_1, model_names=model_names)\nplot.show()","metadata":{},"execution_count":null,"outputs":[]}]}